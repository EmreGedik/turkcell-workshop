{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air reservation system\n",
    "air_reserve = pd.read_csv('../input/air_reserve.csv.gz')\n",
    "air_store_info = pd.read_csv('../input/air_store_info.csv.gz')\n",
    "air_visit_data = pd.read_csv('../input/air_visit_data.csv.gz')\n",
    "\n",
    "# hpg reservation system\n",
    "hpg_reserve = pd.read_csv('../input/hpg_reserve.csv.gz')\n",
    "hpg_store_info = pd.read_csv('../input/hpg_store_info.csv.gz')\n",
    "\n",
    "# additional data\n",
    "store_id_relation = pd.read_csv('../input/store_id_relation.csv.gz')\n",
    "date_info = pd.read_csv('../input/date_info.csv.gz')\n",
    "\n",
    "# test data\n",
    "sample_sub = pd.read_csv('../input/sample_submission.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION =====================================================\n",
    "# TEST DATA ------------------------------------------------------------\n",
    "# transform test data\n",
    "air_test = sample_sub.copy()\n",
    "air_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])\n",
    "air_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])\n",
    "\n",
    "# dataframe for predictions\n",
    "submission_lstm = air_test.copy()\n",
    "\n",
    "# test set for merger with train set\n",
    "air_test = air_test.drop(['id', 'visitors'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ON RESERVATION --------------------------------------------------\n",
    "# combine air and hpg databases\n",
    "hpg_air_reserve = store_id_relation.join(hpg_reserve.set_index('hpg_store_id'), on = 'hpg_store_id')\n",
    "air_reserve_tmp = air_reserve.copy()\n",
    "hpg_air_reserve = hpg_air_reserve.drop('hpg_store_id', axis = 1)\n",
    "reserve = pd.concat([air_reserve_tmp, hpg_air_reserve])\n",
    "\n",
    "# convert columns of \"reserve\" table into datetime format\n",
    "reserve['visit_datetime'] =  pd.to_datetime(reserve['visit_datetime'])\n",
    "reserve['reserve_datetime'] =  pd.to_datetime(reserve['reserve_datetime'])\n",
    "\n",
    "# create column for visit date inside \"reserve\" table\n",
    "reserve['visit_date'] = reserve['visit_datetime'].apply(lambda x: str(x)[0:10])\n",
    "\n",
    "# calculate the gap between visit time and reservation time inside \"reserve\" table\n",
    "reserve['hour_gap'] = reserve['visit_datetime'].sub(reserve['reserve_datetime'])\n",
    "reserve['hour_gap'] = reserve['hour_gap'].apply(lambda x: x/np.timedelta64(1,'h'))\n",
    "\n",
    "# separate reservation into 5 categories based on gap lenght\n",
    "reserve['reserve_-12_h'] = np.where(reserve['hour_gap'] <= 12,\n",
    "                                    reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_12_37_h'] = np.where((reserve['hour_gap'] <= 37) & (reserve['hour_gap'] > 12),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_37_59_h'] = np.where((reserve['hour_gap'] <= 59) & (reserve['hour_gap'] > 37),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_59_85_h'] = np.where((reserve['hour_gap'] <= 85) & (reserve['hour_gap'] > 59),\n",
    "                                       reserve['reserve_visitors'], 0)\n",
    "reserve['reserve_85+_h'] = np.where((reserve['hour_gap'] > 85),\n",
    "                                     reserve['reserve_visitors'], 0)\n",
    "\n",
    "# group by air_store_id and visit_date to enable joining with main table\n",
    "group_list = ['air_store_id', 'visit_date', 'reserve_visitors', 'reserve_-12_h',\n",
    "              'reserve_12_37_h', 'reserve_37_59_h', 'reserve_59_85_h', 'reserve_85+_h']\n",
    "reserve = reserve[group_list].groupby(['air_store_id', 'visit_date'], as_index = False).sum()\n",
    "\n",
    "for i in group_list[2:]:\n",
    "    reserve[i] = reserve[i].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENRE DATA ----------------------------------------------------------\n",
    "# total amount of restaurants of specific genres by area_name\n",
    "air_genres_area = air_store_info.copy()\n",
    "air_genres_area = air_genres_area[['air_store_id', 'air_genre_name', 'air_area_name']].groupby(['air_genre_name', 'air_area_name'],\n",
    "                                                                                              as_index = False).count()\n",
    "air_genres_area = air_genres_area.rename(columns = {'air_store_id': 'genre_in_area'})\n",
    "\n",
    "# total amount of restaurants in area\n",
    "air_area = air_store_info.copy()\n",
    "air_area = air_area[['air_store_id', 'air_area_name']].groupby(['air_area_name'], as_index = False).count()\n",
    "air_area = air_area.rename(columns = {'air_store_id': 'total_r_in_area'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/miniconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# WEEKEND AND HOLIDAYS ------------------------------------------------\n",
    "# additional features for weekends and holidays\n",
    "date_info_mod = date_info.copy()\n",
    "date_info_mod['holiday_eve'] = np.zeros(date_info_mod.shape[0])\n",
    "date_info_mod['holiday_eve'].iloc[:-1] = date_info_mod['holiday_flg'].copy().values[1:]\n",
    "date_info_mod['non_working'] = np.where(date_info_mod['day_of_week'].isin(['Saturday', 'Sunday']) |\n",
    "                                        date_info_mod['holiday_flg'] == 1, 1, 0)\n",
    "date_info_mod = date_info_mod.drop('holiday_flg', axis = 1)\n",
    "\n",
    "# average visitors per restaurant by working and non-working days\n",
    "air_visit_wd = air_visit_data.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')\n",
    "air_visit_wd['visitors'] = air_visit_wd['visitors'].apply(lambda x: np.log1p(x)) \n",
    "\n",
    "# average visitors per restaurant\n",
    "mean_df = air_visit_wd[['visitors',\n",
    "                        'air_store_id',\n",
    "                        'non_working']].copy().groupby(['air_store_id',\n",
    "                                                        'non_working'],\n",
    "                                                        as_index = False).mean()\n",
    "mean_df = mean_df.rename(columns = {'visitors': 'visitors_mean'})\n",
    "\n",
    "\n",
    "# median visitors per restaurant\n",
    "median_df = air_visit_wd[['visitors',\n",
    "                          'air_store_id',\n",
    "                          'non_working']].copy().groupby(['air_store_id',\n",
    "                                                          'non_working'],\n",
    "                                                          as_index = False).median()\n",
    "median_df = median_df.rename(columns = {'visitors': 'visitors_median'})\n",
    "\n",
    "# max visitors per restaurant\n",
    "max_df = air_visit_wd[['visitors',\n",
    "                       'air_store_id',\n",
    "                       'non_working']].copy().groupby(['air_store_id',\n",
    "                                                       'non_working'],\n",
    "                                                       as_index = False).max()\n",
    "max_df = max_df.rename(columns = {'visitors': 'visitors_max'})\n",
    "\n",
    "# min visitors per restaurant\n",
    "min_df = air_visit_wd[['visitors',\n",
    "                       'air_store_id',\n",
    "                       'non_working']].copy().groupby(['air_store_id',\n",
    "                                                       'non_working'],\n",
    "                                                       as_index = False).min()\n",
    "min_df = min_df.rename(columns = {'visitors': 'visitors_min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN TABLES INTO TRAINING AND TEST SETS ----------------------------\n",
    "# function for combining train/test dataset with additional information\n",
    "def merge_join(df):\n",
    "    # add month of visit\n",
    "    df['month'] = df['visit_date'].apply(lambda x: float(str(x)[5:7]))\n",
    "\n",
    "    # add weekday and holiday flag\n",
    "    df = df.join(date_info_mod.set_index('calendar_date'), on = 'visit_date')\n",
    "\n",
    "\n",
    "    # add genre and area name)\n",
    "    df = df.join(air_store_info.set_index('air_store_id'), on = 'air_store_id')\n",
    "\n",
    "    # add quantity of same genre in area\n",
    "    df = pd.merge(df, air_genres_area, how = 'left',\n",
    "                  left_on = ['air_genre_name', 'air_area_name'],\n",
    "                  right_on = ['air_genre_name', 'air_area_name'])\n",
    "\n",
    "\n",
    "    # add total quatity of restaurants in area\n",
    "    df = pd.merge(df, air_area, how = 'left',\n",
    "                  left_on = ['air_area_name'],\n",
    "                  right_on = ['air_area_name'])\n",
    "\n",
    "    # add reservation information\n",
    "    df = pd.merge(df, reserve, how = 'left',\n",
    "                  left_on = ['air_store_id', 'visit_date'],\n",
    "                  right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "    # add visitors number mean, median, max and min per each restaurant\n",
    "    df = pd.merge(df, mean_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "\n",
    "    df = pd.merge(df, median_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    df = pd.merge(df, max_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    df = pd.merge(df, min_df, how = 'left',\n",
    "                  left_on = ['air_store_id', 'non_working'],\n",
    "                  right_on = ['air_store_id', 'non_working'])\n",
    "    \n",
    "    # change NaN to 0\n",
    "    df = df.fillna(0) \n",
    "   \n",
    "    return df\n",
    "\n",
    "# combine train/test data with additional information\n",
    "air_train = air_visit_data.copy()\n",
    "X = merge_join(air_train)\n",
    "X_test = merge_join(air_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE STRING FEATURES ----------------------------------------------\n",
    "# (one-hot encoding may provide better result,\n",
    "# I preferred to apply labels encoding to avoid high dimensional feature space)\n",
    "\n",
    "# Weekday\n",
    "le_weekday = preprocessing.LabelEncoder()\n",
    "le_weekday.fit(X['day_of_week'])\n",
    "X['day_of_week'] = le_weekday.transform(X['day_of_week'])\n",
    "X_test['day_of_week'] = le_weekday.transform(X_test['day_of_week'])\n",
    "\n",
    "# Genre name\n",
    "le_genre = preprocessing.LabelEncoder()\n",
    "le_genre.fit(X['air_genre_name'])\n",
    "X['air_genre_name'] = le_genre.transform(X['air_genre_name'])\n",
    "X_test['air_genre_name'] = le_genre.transform(X_test['air_genre_name'])\n",
    "\n",
    "# Area name\n",
    "le_area = preprocessing.LabelEncoder()\n",
    "le_area.fit(X['air_area_name'])\n",
    "X['air_area_name'] = le_area.transform(X['air_area_name'])\n",
    "X_test['air_area_name'] = le_area.transform(X_test['air_area_name'])\n",
    "\n",
    "# id\n",
    "le_id = preprocessing.LabelEncoder()\n",
    "le_id.fit(X['air_store_id'])\n",
    "X['air_store_id'] = le_id.transform(X['air_store_id'])\n",
    "X_test['air_store_id'] = le_id.transform(X_test['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/miniconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "/home/deep/miniconda3/envs/tensorflow_gpuenv/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# SIMULTANEOUS TRANSFORMATION OF TRAIN AND TEST SETS -------------------\n",
    "# combine train and test sets\n",
    "X_all = X.append(X_test)\n",
    "\n",
    "# date table (includes all dates for training and test period)\n",
    "dates = np.arange(np.datetime64(X_all.visit_date.min()),\n",
    "                  np.datetime64(X_all.visit_date.max()) + 1,\n",
    "                  datetime.timedelta(days=1))\n",
    "ids = X_all['air_store_id'].unique()\n",
    "dates_all = dates.tolist()*len(ids)\n",
    "ids_all = np.repeat(ids, len(dates.tolist())).tolist()\n",
    "df_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\n",
    "df_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])\n",
    "\n",
    "# create copy of X_all with data relevant to 'visit_date'\n",
    "X_dates = X_all[['visit_date', 'month', 'day_of_week', 'holiday_eve', 'non_working']].copy()\n",
    "\n",
    "# remove duplicates to avoid memory issues\n",
    "X_dates = X_dates.drop_duplicates('visit_date')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with information about each date\n",
    "df_to_reshape = df_all.merge(X_dates,\n",
    "                             how = \"left\",\n",
    "                             left_on = 'visit_date',\n",
    "                             right_on = 'visit_date')\n",
    "\n",
    "# create copy of X_all with data relevant to 'air_store_id'\n",
    "X_stores = X_all[['air_store_id', 'air_genre_name', 'air_area_name', 'latitude',\n",
    "                  'longitude', 'genre_in_area', 'total_r_in_area']].copy()       \n",
    "\n",
    "# remove duplicates to avoid memory issues\n",
    "X_stores = X_stores.drop_duplicates('air_store_id')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with information about each restaurant\n",
    "df_to_reshape = df_to_reshape.merge(X_stores,\n",
    "                                    how = \"left\",\n",
    "                                    left_on = 'air_store_id',\n",
    "                                    right_on = 'air_store_id')\n",
    "\n",
    "# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\n",
    "df_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date', 'reserve_visitors', 'visitors_mean', \n",
    "                                       'visitors_median', 'visitors_max', 'visitors_min', 'visitors']],\n",
    "                                    how = \"left\",\n",
    "                                    left_on = ['air_store_id', 'visit_date'],\n",
    "                                    right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "# separate 'visitors' into output array\n",
    "Y_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n",
    "\n",
    "# take log(y+1)\n",
    "Y_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n",
    "\n",
    "# add flag for days when a restaurant was closed\n",
    "df_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n",
    "                                        df_to_reshape['visit_date'].isin(X['visit_date']).values,1,0)\n",
    "\n",
    "# drop 'visitors' and from dataset\n",
    "df_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n",
    "\n",
    "# fill in NaN values\n",
    "df_to_reshape = df_to_reshape.fillna(-1)\n",
    "\n",
    "# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\n",
    "columns_list = [x for x in list(df_to_reshape.iloc[:,2:])]\n",
    "\n",
    "# bound all numerical values between -1 and 1\n",
    "# note: to avoid data leakage 'fit' should be made on traid data and 'transform' on train and test data\n",
    "# in this case all data in test set is taken from train set, thus fit/transform on all data \n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(df_to_reshape[columns_list])\n",
    "df_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_minus shape (829, 478, 7)\n"
     ]
    }
   ],
   "source": [
    "# SPECIFIC PREPARATION FOR NEURAL NETWORK AND ENCODER/DECODER ---------------\n",
    "# reshape X into (samples, timesteps, features)\n",
    "X_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n",
    "                                                len(dates),\n",
    "                                                df_to_reshape.shape[1]-2)\n",
    "\n",
    "# isolate output for train set and reshape it for time series\n",
    "Y_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(X['visit_date'].values) &\n",
    "                          Y_lstm_df['air_store_id'].isin(X['air_store_id'].values),]\n",
    "Y_lstm = Y_lstm_df.values[:,2].reshape(len(X['air_store_id'].unique()),\n",
    "                                       len(X['visit_date'].unique()),\n",
    "                                       1)\n",
    "\n",
    "# test dates\n",
    "n_test_dates = len(X_test['visit_date'].unique())\n",
    "\n",
    "# make additional features for number of visitors in t-1, t-2, ... t-7\n",
    "t_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\n",
    "for i in range(1,8):\n",
    "    temp = Y_lstm.copy()\n",
    "    temp[:,i:,:] = Y_lstm[:,0:-i,:].copy()\n",
    "    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\n",
    "t_minus = t_minus[:,:,1:]\n",
    "print (\"t_minus shape\", t_minus.shape)\n",
    "\n",
    "\n",
    "# split X_all into training and test data\n",
    "X_lstm = X_all_lstm[:,:-n_test_dates,:]\n",
    "X_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n",
    "\n",
    "# add t-1, t-2 ... t-7 visitors to feature vector\n",
    "X_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_to_reshape.loc[df_to_reshape['visit_date'].isin(X_test['visit_date'].values) &\n",
    "                         df_to_reshape['air_store_id'].isin(X['air_store_id'].values),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez_compressed('processed_input/train_features_lstm.npz', X_lstm=X_lstm, Y_lstm=Y_lstm)\n",
    "np.savez_compressed('processed_input/test_features_lstm.npz', X_lstm_test=X_lstm_test, air_store_id=le_id.inverse_transform(test['air_store_id']))\n",
    "\n",
    "submission_lstm.to_csv('processed_input/submission_lstm.csv.gz', compression='gzip', index=False)\n",
    "df_to_reshape.to_csv('processed_input/prep_submission_lstm.csv.gz', compression='gzip', index=False)\n",
    "X.to_csv('processed_input/train_lstm.csv.gz', compression='gzip', index=False)\n",
    "X_test.to_csv('processed_input/test_lstm.csv.gz', compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (TensorFlow GPU)",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
