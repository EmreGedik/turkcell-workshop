{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train = np.load('processed_input/train_features_lstm.npz')\n",
    "lstm_test = np.load('processed_input/test_features_lstm.npz')\n",
    "\n",
    "X = pd.read_csv('processed_input/train_lstm.csv.gz')\n",
    "X_test = pd.read_csv('processed_input/test_lstm.csv.gz')\n",
    "df_to_reshape = pd.read_csv('processed_input/prep_submission_lstm.csv.gz')\n",
    "submission_lstm = pd.read_csv('processed_input/submission_lstm.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot Uniform\n",
    "\n",
    "*Glorot normal initializer, also called Xavier normal initializer.*\n",
    "\n",
    "## Dropout\n",
    "\n",
    "***“to prevent over-fitting”***\n",
    "\n",
    "![](appendix/dropout.png)\n",
    "\n",
    "*Source: Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014*\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "*Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.*\n",
    "\n",
    "![](appendix/rmsprop.gif)\n",
    "\n",
    "*Source: http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif*\n",
    "\n",
    "## LSTM Encoder & Decoder Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = lstm_train['X_lstm'].shape[2]\n",
    "latent_dim = 256\n",
    "\n",
    "# encoder training\n",
    "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, \n",
    "               batch_input_shape = (1, None, num_encoder_tokens),\n",
    "               stateful = False,\n",
    "               return_sequences = True,\n",
    "               return_state = True,\n",
    "               recurrent_initializer = 'glorot_uniform')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c] # 'encoder_outputs' are ignored and only states are kept.\n",
    "\n",
    "# Decoder training, using 'encoder_states' as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "decoder_lstm_1 = LSTM(latent_dim,\n",
    "                      batch_input_shape = (1, None, num_encoder_tokens),\n",
    "                      stateful = False,\n",
    "                      return_sequences = True,\n",
    "                      return_state = False,\n",
    "                      dropout = 0.2,\n",
    "                      recurrent_dropout = 0.2) # True\n",
    "\n",
    "decoder_lstm_2 = LSTM(128,\n",
    "                     stateful = False,\n",
    "                     return_sequences = True,\n",
    "                     return_state = True,\n",
    "                     dropout = 0.2,\n",
    "                     recurrent_dropout = 0.2)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm_2(decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\n",
    "decoder_dense = TimeDistributed(Dense(lstm_train['Y_lstm'].shape[2], activation = 'relu'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# training model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "training_model.compile(optimizer = 'rmsprop', loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "829/829 [==============================] - 168s 202ms/step - loss: 0.2623\n",
      "Epoch 2/6\n",
      "829/829 [==============================] - 166s 200ms/step - loss: 0.1688\n",
      "Epoch 3/6\n",
      "829/829 [==============================] - 166s 200ms/step - loss: 0.1418\n",
      "Epoch 4/6\n",
      "829/829 [==============================] - 166s 200ms/step - loss: 0.1259\n",
      "Epoch 5/6\n",
      "829/829 [==============================] - 166s 200ms/step - loss: 0.1129\n",
      "Epoch 6/6\n",
      "829/829 [==============================] - 166s 200ms/step - loss: 0.1080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42485eedd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n",
    "# generator that randomly creates times series of 39 consecutive days\n",
    "# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \n",
    "def dec_enc_n_days_gen(X_3d, Y_3d, length):\n",
    "    while 1:\n",
    "        decoder_boundary = X_3d.shape[1] - length - 1\n",
    "        \n",
    "        encoder_start = np.random.randint(0, decoder_boundary)\n",
    "        encoder_end = encoder_start + length\n",
    "        \n",
    "        decoder_start = encoder_start + 1\n",
    "        decoder_end = encoder_end + 1\n",
    "        \n",
    "        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n",
    "        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n",
    "        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n",
    "        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n",
    "        \n",
    "        yield([X_to_conc,\n",
    "               X_to_decode],\n",
    "               Y_decoder)\n",
    "\n",
    "training_model.fit_generator(dec_enc_n_days_gen(lstm_train['X_lstm'][:,:,:], lstm_train['Y_lstm'][:,:,:], 39),\n",
    "                            steps_per_epoch = lstm_train['X_lstm'][:,:,:].shape[0],\n",
    "                            verbose = 1,\n",
    "                            epochs = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Mechanism\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION FUNCTION --------------------------------------------------\n",
    "\n",
    "# function takes 39 days before first prediction day (input_seq)\n",
    "# then using encoder to identify hidden states for these 39 days.\n",
    "# Next, decoder takes hidden states provided by encoder\n",
    "# and predicts number of visitors from day 2 to day 40.\n",
    "# Day 40 is the first day of target_seq.\n",
    "\n",
    "# Predicted value for day 40 is appended to features of day 41.\n",
    "# Then function takes period from day 2 to day 40 and repeat the process\n",
    "# unil all days in target sequence get their predictions. \n",
    "\n",
    "# The output of the function is the vector with predictions that has\n",
    "# following shape: 820 restaurants * 39 days * 1 predicted visitors amount\n",
    "\n",
    "def predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n",
    "    # state of input sequence produced by encoder\n",
    "    state = inf_enc.predict(input_seq)\n",
    "    \n",
    "    # restrict target sequence to the same shape as X_lstm_test\n",
    "    target_seq = target_seq[:,:, :lstm_test['X_lstm_test'].shape[2]]\n",
    "    \n",
    "    \n",
    "    # create vector that contains y for previous 7 days\n",
    "    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, lstm_test['X_lstm_test'].shape[2]:-1]), axis = 2)\n",
    "    \n",
    "    # current sequence that is going to be modified each iteration of the prediction loop\n",
    "    current_seq = input_seq.copy()\n",
    "    \n",
    "    \n",
    "    # predicting outputs\n",
    "    output = np.ones([target_seq.shape[0],1,1])\n",
    "    for i in range(target_seq.shape[1]):\n",
    "        # add visitors for previous 7 days into features of a new day\n",
    "        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n",
    "        \n",
    "        # move prediction window one day forward\n",
    "        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n",
    "        \n",
    "        \n",
    "        # predict visitors amount\n",
    "        pred = inf_dec.predict([current_seq] + state)\n",
    "        \n",
    "        # update t_minus_seq\n",
    "        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n",
    "        t_minus_seq = t_minus_seq[:,:,:-1]        \n",
    "        \n",
    "        # update predicitons list\n",
    "        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n",
    "        \n",
    "        # update state\n",
    "        state = inf_enc.predict(current_seq)\n",
    "    \n",
    "    return output[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE ENCODER AND DECODER -----------------------------------------    \n",
    "# inference encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# inference decoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n",
    "                                                    initial_state = decoder_states_inputs))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Submission & Submiting with Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting test values\n",
    "enc_dec_pred = predict_sequence(encoder_model,\n",
    "                                decoder_model,\n",
    "                                lstm_train['X_lstm'][:,-lstm_test['X_lstm_test'].shape[1]:,:],\n",
    "                                lstm_train['Y_lstm'][:,-lstm_test['X_lstm_test'].shape[1]:,:],\n",
    "                                lstm_test['X_lstm_test'][:,:,:])\n",
    "\n",
    "# Add predicted test values to submission dataset ---------------------\n",
    "\n",
    "# Note: it is important to preserve the order of time series.\n",
    "# Thus, test set will contain all 829 lines in the same order as train set.\n",
    "# To make this 'air_store_id' is taken as in X and not in X_test (second line of 'test' variable below).\n",
    "# Only relevant results will be merged for submission dataframe\n",
    "test = df_to_reshape.loc[df_to_reshape['visit_date'].isin(X_test['visit_date'].values) &\n",
    "                         df_to_reshape['air_store_id'].isin(X['air_store_id'].values),]\n",
    "\n",
    "\n",
    "# reshape predicted values to initial shape\n",
    "test_pred = enc_dec_pred.reshape(test.shape[0], 1)\n",
    "test_pred_exp = np.exp(test_pred) - 1.0\n",
    "test_pred_exp[test_pred_exp<0] = 0\n",
    "\n",
    "# add predictions to dataframe with 'air_store_id' and 'visit_date'\n",
    "test_df_pred = test[['air_store_id', 'visit_date']].copy()\n",
    "test_df_pred['predicted'] = test_pred_exp\n",
    "\n",
    "# reverse transform of 'air_store_id'\n",
    "test_df_pred['air_store_id'] = lstm_test['air_store_id']\n",
    "\n",
    "# finalizing submission csv file\n",
    "submission_df = submission_lstm.merge(test_df_pred,\n",
    "                                     how = 'left',\n",
    "                                     left_on = ['air_store_id', 'visit_date'],\n",
    "                                     right_on = ['air_store_id', 'visit_date'])\n",
    "\n",
    "submission_df['visitors'] = submission_df['predicted']\n",
    "submission_df = submission_df.drop(['air_store_id', 'visit_date', 'predicted'], axis = 1)\n",
    "submission_df.to_csv('submissions/lstm_submission.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c recruit-restaurant-visitor-forecasting -f submissions/lstm_submission.csv.gz -m \"Final LSTM\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Encoder-Decoder Model\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "- http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n",
    "- https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
    "- https://github.com/Arturus/kaggle-web-traffic\n",
    "\n",
    "- https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\n",
    "- http://ruder.io/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
