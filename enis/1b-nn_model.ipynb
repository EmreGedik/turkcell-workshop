{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "dd50fef0-4659-49ce-9e19-a8f3c16b9542",
    "_uuid": "6ae68db95c99261f9d9235185aaaf2344c335169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.layers import Embedding, Input, Dense\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "568d6277-ebda-45dd-abbe-6a1724aff6ac",
    "_uuid": "02a795ac0f813434881bc84f56d9f6bfb99f8d47"
   },
   "outputs": [],
   "source": [
    "nn_train = np.load('processed_input/train_features_nn.npz')\n",
    "nn_test = np.load('processed_input/test_features_nn.npz')\n",
    "\n",
    "train = pd.read_csv('processed_input/train_attr_nn.csv.gz')\n",
    "test = pd.read_csv('processed_input/test_attr_nn.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function implements the Keras neural network model.\n",
    "\n",
    "Basic structure:\n",
    "- categorical columns get independent inputs, passed through embedding layer and then flattened.\n",
    "- numeric columns are simply taken as float32 inputs\n",
    "- the final tensors of categorical and numerical are then concatenated together\n",
    "- following the concatenated layer and simple feed forward neural network is implemented.\n",
    "- output layer has 'ReLU' activation function\n",
    "\n",
    "## Embedding for Categorical Features\n",
    "\n",
    "*Key sentence:* Embeddings **capture richer relationships and complexities than the raw categories.**\n",
    "\n",
    "*A key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for relationships between categories to be captured. Perhaps Saturday and Sunday have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status.*\n",
    "\n",
    "## Concatenation of Layers\n",
    "\n",
    "We can concatenate different sub-networks. In this example, we concatenate embedding layers.\n",
    "\n",
    "## Leaky ReLU (Activation Function)\n",
    "\n",
    "ReLU             |  Leaky ReLU\n",
    ":-------------------------:|:-------------------------:\n",
    " ![](appendix/relu.png) | ![](appendix/leaky_relu.png) \n",
    " *Source: https://sefiks.com/wp-content/uploads/2017/08/relu.png* | *Source: https://sefiks.com/wp-content/uploads/2018/02/leaky-relu.png*\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "*We normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.*\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "*Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.*\n",
    "\n",
    "![](appendix/rmsprop.gif)\n",
    "\n",
    "*Source: http://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif*\n",
    "\n",
    "## Our Model\n",
    "\n",
    "![](appendix/nn_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "282b5af0-fcfc-419e-9afa-6ca86756173c",
    "_uuid": "6df537d60eca75ceb2445359aa67ba8c1688ccda"
   },
   "outputs": [],
   "source": [
    "def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Categorical Embeddings\n",
    "    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n",
    "    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n",
    "                                 name='air_store_id_emb')(air_store_id)\n",
    "    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n",
    "\n",
    "    dow = Input(shape=(1,), dtype='int32', name='dow')\n",
    "    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n",
    "    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n",
    "\n",
    "    month = Input(shape=(1,), dtype='int32', name='month')\n",
    "    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n",
    "    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n",
    "\n",
    "    air_area_name, air_genre_name = [], []\n",
    "    air_area_name_emb, air_genre_name_emb = [], []\n",
    "    for i in range(7):\n",
    "        area_name_col = 'air_area_name' + str(i)\n",
    "        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n",
    "        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_name_col + '_emb')(air_area_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n",
    "        air_area_name_emb.append(tmp)\n",
    "\n",
    "        if i > 4:\n",
    "            continue\n",
    "        area_genre_col = 'air_genre_name' + str(i)\n",
    "        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n",
    "        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_genre_col + '_emb')(air_genre_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n",
    "        air_genre_name_emb.append(tmp)\n",
    "\n",
    "    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n",
    "    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n",
    "\n",
    "    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n",
    "    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n",
    "    \n",
    "    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n",
    "    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n",
    "    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n",
    "    \n",
    "    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n",
    "    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n",
    "                                   name='air_genre_code_emb')(air_genre_code)\n",
    "    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n",
    "\n",
    "    # Float Attributes\n",
    "    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n",
    "    year = Input(shape=(1,), dtype='float32', name='year')\n",
    "    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n",
    "    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n",
    "    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n",
    "    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n",
    "    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n",
    "    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n",
    "    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n",
    "    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n",
    "    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n",
    "    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n",
    "    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n",
    "    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n",
    "    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n",
    "    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n",
    "    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n",
    "    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n",
    "    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n",
    "    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n",
    "    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n",
    "    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n",
    "    \n",
    "    # Date attributes Embedding\n",
    "    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n",
    "    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n",
    "\n",
    "    # Concatenate All Layers\n",
    "    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n",
    "                    median_visitors, max_visitors, count_observations, rs1_x, rv1_x,\n",
    "                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n",
    "                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n",
    "                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "                    date_emb, air_area_name_emb, air_genre_name_emb,\n",
    "                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n",
    "\n",
    "    # Top Layer\n",
    "    m = Dense(hidden1_neurons, name='hidden1',\n",
    "             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n",
    "                            stddev=0.05, seed=None))(cat_layer)\n",
    "    m = keras.layers.LeakyReLU(alpha=0.2)(m)\n",
    "    m = keras.layers.BatchNormalization()(m)\n",
    "    \n",
    "    m1 = Dense(hidden2_neurons, name='hidden2')(m)\n",
    "    m1 = keras.layers.LeakyReLU(alpha=0.2)(m1)\n",
    "    m = Dense(1, activation='relu')(m1)\n",
    "\n",
    "    # Input Tensor\n",
    "    inp_ten = [\n",
    "        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, count_observations,\n",
    "        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n",
    "        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "        dow, year, month, air_store_id, air_area_code, air_genre_code\n",
    "    ]\n",
    "    inp_ten += air_area_name\n",
    "    inp_ten += air_genre_name\n",
    "    \n",
    "    # Construct NN Model\n",
    "    model = Model(inp_ten, m)\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "45f736a7-30df-4339-a803-251e6e818734",
    "_uuid": "513635a989d672572fb3e30740ebc33785beade7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 15s 70us/step - loss: 0.2542 - acc: 0.0000e+00 - val_loss: 0.2646 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 14s 67us/step - loss: 0.2525 - acc: 0.0000e+00 - val_loss: 0.2710 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 14s 65us/step - loss: 0.2517 - acc: 0.0000e+00 - val_loss: 0.2664 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 14s 66us/step - loss: 0.2462 - acc: 0.0000e+00 - val_loss: 0.2559 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 15s 71us/step - loss: 0.2456 - acc: 0.0000e+00 - val_loss: 0.2488 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 16s 76us/step - loss: 0.2450 - acc: 0.0000e+00 - val_loss: 0.2526 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 14s 68us/step - loss: 0.2410 - acc: 0.0000e+00 - val_loss: 0.2434 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 14s 67us/step - loss: 0.2404 - acc: 0.0000e+00 - val_loss: 0.2625 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 15s 71us/step - loss: 0.2400 - acc: 0.0000e+00 - val_loss: 0.2540 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 15s 72us/step - loss: 0.2383 - acc: 0.0000e+00 - val_loss: 0.2506 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 15s 70us/step - loss: 0.2380 - acc: 0.0000e+00 - val_loss: 0.2474 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 14s 67us/step - loss: 0.2377 - acc: 0.0000e+00 - val_loss: 0.2435 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 15s 68us/step - loss: 0.2366 - acc: 0.0000e+00 - val_loss: 0.2436 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 15s 72us/step - loss: 0.2365 - acc: 0.0000e+00 - val_loss: 0.2419 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 15s 73us/step - loss: 0.2362 - acc: 0.0000e+00 - val_loss: 0.2389 - val_acc: 0.0000e+00\n",
      "Model trained\n",
      "Model predictions done.\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_complete_model(train, hidden1_neurons=45)\n",
    "\n",
    "for i in range(5):\n",
    "    model.fit(\n",
    "        nn_train['X_train'].tolist(), \n",
    "        nn_train['Y_train'], \n",
    "        epochs=8, \n",
    "        verbose=0, \n",
    "        batch_size=512, \n",
    "        shuffle=True\n",
    "    )\n",
    "    model.fit(\n",
    "        nn_train['X_train'].tolist(), \n",
    "        nn_train['Y_train'], \n",
    "        epochs=3, \n",
    "        verbose=1, \n",
    "        batch_size=512, \n",
    "        shuffle=True, \n",
    "        validation_split=0.15\n",
    "    )\n",
    "    \n",
    "model.fit(\n",
    "    nn_train['X_train'].tolist(),\n",
    "    nn_train['Y_train'], \n",
    "    epochs=4, \n",
    "    verbose=0, \n",
    "    batch_size=512, \n",
    "    shuffle=True\n",
    ")\n",
    "print(\"Model trained\")\n",
    "\n",
    "preds = pd.Series(model.predict(nn_test['X_test'].tolist()).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "test['visitors'] = preds\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "sub1['preds'] = pd.Series(preds)\n",
    "print(\"Model predictions done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a70c7bfa-fb9b-46bd-a198-7e81ea5aa523",
    "_uuid": "058e76d4e0b3797e0b2fcfedc9613e67b7557ab7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv.gz', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../input/*.csv.gz')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "    \n",
    "date_info = pd.read_csv('processed_input/date_info_nn.csv.gz')\n",
    "air_visit_data = pd.read_csv('processed_input/train_nn.csv.gz')\n",
    "sample_submission = pd.read_csv('processed_input/test_nn.csv.gz')\n",
    "    \n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='visit_date', how='left')\n",
    "visit_data.drop('visit_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='visit_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub2 = sub2.fillna(-1) # for the unfound values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Submission & Submiting with Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "051fdf64-06fe-4bac-a5a1-ac31d13fbc08",
    "_uuid": "f32a1d98c3a8d5ad90f5db9a6ea8c723ec7b8dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def final_visitors(x, alt=False):\n",
    "    visitors_x, visitors_y = x['visitors_x'], x['visitors_y']\n",
    "    if x['visitors_y'] == -1:\n",
    "        return visitors_x\n",
    "    else:\n",
    "        return 0.7*visitors_x + 0.3*visitors_y* 1.3\n",
    "\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "sub_merge['visitors'] = sub_merge.apply(lambda x: final_visitors(x), axis=1)\n",
    "print(\"Done\")\n",
    "sub_merge[['id', 'visitors']].to_csv('submissions/nn_submission.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c recruit-restaurant-visitor-forecasting -f submissions/nn_submission.csv.gz -m \"Final NN\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('nn_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.fast.ai/2018/04/29/categorical-embeddings/\n",
    "- https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "- https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
    "- http://ruder.io/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
