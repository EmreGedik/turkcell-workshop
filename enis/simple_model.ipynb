{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basit Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Girdileri Okuma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('../input/air_visit_data.csv.gz'),\n",
    "    'as': pd.read_csv('../input/air_store_info.csv.gz'),\n",
    "    'hs': pd.read_csv('../input/hpg_store_info.csv.gz'),\n",
    "    'ar': pd.read_csv('../input/air_reserve.csv.gz'),\n",
    "    'hr': pd.read_csv('../input/hpg_reserve.csv.gz'),\n",
    "    'id': pd.read_csv('../input/store_id_relation.csv.gz'),\n",
    "    'tes': pd.read_csv('../input/sample_submission.csv.gz'),\n",
    "    'hol': pd.read_csv('../input/date_info.csv.gz').rename(columns={'calendar_date':'visit_date'})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Air Reserve ve HPG Reserve verilerinin birlestirilmesi\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - Visit datetime string tipinden datetime tipine cevirme\n",
    "    - Visit datetime verisinden Day Of Week bilgisini cikarma\n",
    "    - Reverse datetime string tipinden datetime tipine cevirme\n",
    "    - Reserve ile visit arasindaki zaman farkini ekleme\n",
    "    - Exclude ???\n",
    "    - \n",
    "\"\"\"\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    # Exclude same-week reservations simple logic\n",
    "    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']]\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Training Verisinin Hazirlanmasi:\n",
    "        - Visit date string tipinden datetime tipine cevirme\n",
    "        - Day of Week ve Day of Year bilgilerinin visit date bilgilerinin cikartirilmasi\n",
    "        - yil, ay, ve hafta bilgilerinin visit date verisinden cikartirilmasi\n",
    "\"\"\"\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['doy'] = data['tra']['visit_date'].dt.dayofyear\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['week'] = data['tra']['visit_date'].dt.week\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test Verisinin Hazirlanmasi:\n",
    "        - Visit date string tipinden datetime tipine cevirme\n",
    "        - Day of Week ve Day of Year bilgilerinin visit date bilgilerinin cikartirilmasi\n",
    "        - yil, ay, ve hafta bilgilerinin visit date verisinden cikartirilmasi\n",
    "\"\"\"\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['doy'] = data['tes']['visit_date'].dt.dayofyear\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['week'] = data['tes']['visit_date'].dt.week\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Tekil restorantlarin bilgilerinin cikartilmasi\n",
    "\"\"\"\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# Basit istatistiksel verilerin stores tablosuna eklenmesi\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "\n",
    "# Genre ve Area verilerinin basit bir label encoderdan gecirilip stores tablosuna eklenmesi\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "# Orjinal Genre ve Area verilerinin ayni label encoderdan gecirilip stores tablosuna eklenmesi\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tatil verisine day of week verisinin eklenmesi\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "\n",
    "# Test ve train tablolarina tatil verisinin eklenmesi\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores tablosunun test ve train tablolarina eklenmesi\n",
    "train = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rezerasyon bilgilerinin test ve train tablosuna eklenmesi\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Rezervasyon sayilarinin ortalamalarinin tablolara eklenmesi\n",
    "\"\"\"\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM JMBULL\n",
    "\n",
    "# Konum bilgisinin train ve test tablolarina eklenmesi\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "# Enlem ve boylam verilerinin degerlerinin toplanip yeni bir feature cikarimi\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "# Store idlere Label encoder uygulanmasi\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\n",
    "# Bos alanlarin -1 ile doldurulmasi\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yarisma metriginin tanimlanmasi\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test data prepared\n"
     ]
    }
   ],
   "source": [
    "value_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors', 'max_visitors', \n",
    "             'count_observations', \n",
    "             'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y',\n",
    "             'total_reserv_sum','total_reserv_mean',\n",
    "             'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n",
    "\n",
    "# Basit nn icerisinde kullanilacak sutunlarin belirlenmesi\n",
    "nn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n",
    "                      'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n",
    "                      'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n",
    "                      'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n",
    "\n",
    "X = train.copy()\n",
    "X_test = test[nn_col].copy()\n",
    "\n",
    "# Sayisal veri iceren verilerin standardize edilmesi\n",
    "value_scaler = preprocessing.MinMaxScaler()\n",
    "for vcol in value_col:\n",
    "    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n",
    "\n",
    "# Train verisinin hazirlanmasi\n",
    "X_train = list(X[nn_col].T.values)\n",
    "Y_train = np.log1p(X['visitors']).values\n",
    "nn_train = [X_train, Y_train]\n",
    "nn_test = [list(X_test[nn_col].T.values)]\n",
    "print(\"Train and test data prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n",
    "    # ======================= Air Store Id Embedding ======================= #\n",
    "    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n",
    "    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n",
    "                                 name='air_store_id_emb')(air_store_id)\n",
    "    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # ======================== Day of Week Embedding ======================== #\n",
    "    dow = Input(shape=(1,), dtype='int32', name='dow')\n",
    "    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n",
    "    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # ======================= Month of Week Embedding ======================= #\n",
    "    month = Input(shape=(1,), dtype='int32', name='month')\n",
    "    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n",
    "    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # ======================= Genre ve Area Embedding ======================= #\n",
    "    air_area_name, air_genre_name = [], []\n",
    "    air_area_name_emb, air_genre_name_emb = [], []\n",
    "    for i in range(7):\n",
    "        area_name_col = 'air_area_name' + str(i)\n",
    "        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n",
    "        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_name_col + '_emb')(air_area_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n",
    "        air_area_name_emb.append(tmp)\n",
    "\n",
    "        if i > 4:\n",
    "            continue\n",
    "        area_genre_col = 'air_genre_name' + str(i)\n",
    "        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n",
    "        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n",
    "                        name=area_genre_col + '_emb')(air_genre_name[-1])\n",
    "        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n",
    "        air_genre_name_emb.append(tmp)\n",
    "\n",
    "    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n",
    "    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n",
    "\n",
    "    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n",
    "    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "    \n",
    "    # ======================== Area Code Embedding ======================== #\n",
    "    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n",
    "    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n",
    "    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n",
    "    \n",
    "    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n",
    "    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n",
    "                                   name='air_genre_code_emb')(air_genre_code)\n",
    "    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # ======================= Float Fields Embedding ====================== #\n",
    "    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n",
    "    year = Input(shape=(1,), dtype='float32', name='year')\n",
    "    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n",
    "    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n",
    "    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n",
    "    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n",
    "    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # =================== Reservation Statistics ========================== #\n",
    "    \n",
    "    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n",
    "    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n",
    "    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n",
    "    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n",
    "    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n",
    "    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n",
    "    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n",
    "    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n",
    "    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n",
    "    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n",
    "    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n",
    "    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n",
    "    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n",
    "    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n",
    "    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n",
    "    \n",
    "    # ===================================================================== #\n",
    "    \n",
    "    # ======================= Date Embeddings ============================= #\n",
    "    \n",
    "    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n",
    "    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "    \n",
    "    # ============================ Concatenate =========================== #\n",
    "\n",
    "    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n",
    "                    median_visitors, max_visitors, \n",
    "                    count_observations, rs1_x, rv1_x,\n",
    "                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n",
    "                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n",
    "                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "                    date_emb, air_area_name_emb, air_genre_name_emb,\n",
    "                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    # ============================= Top Model ============================= #\n",
    "    \n",
    "    m = Dense(hidden1_neurons, name='hidden1',\n",
    "             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n",
    "                            stddev=0.05, seed=None))(cat_layer)\n",
    "    m = keras.layers.PReLU()(m)\n",
    "    m = keras.layers.BatchNormalization()(m)\n",
    "    \n",
    "    m1 = Dense(hidden2_neurons, name='sub1')(m)\n",
    "    m1 = keras.layers.PReLU()(m1)\n",
    "    m = Dense(1, activation='relu')(m1)\n",
    "    \n",
    "    # ===================================================================== #\n",
    "    \n",
    "    # =========================== Model Build ============================= #\n",
    "\n",
    "    inp_ten = [\n",
    "        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, \n",
    "        count_observations,\n",
    "        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n",
    "        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n",
    "        dow, year, month, air_store_id, air_area_code, air_genre_code\n",
    "    ]\n",
    "    inp_ten += air_area_name\n",
    "    inp_ten += air_genre_name\n",
    "    model = keras.Model(inp_ten, m)\n",
    "    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n",
    "    \n",
    "    # ===================================================================== #\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enis/miniconda3/envs/tf_env/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 15s 71us/step - loss: 0.5118 - acc: 0.0000e+00 - val_loss: 0.3942 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 12s 57us/step - loss: 0.2680 - acc: 0.0000e+00 - val_loss: 0.6732 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 12s 57us/step - loss: 0.2599 - acc: 0.0000e+00 - val_loss: 0.3460 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 21s 96us/step - loss: 0.2501 - acc: 0.0000e+00 - val_loss: 0.2857 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 20s 96us/step - loss: 0.2504 - acc: 0.0000e+00 - val_loss: 0.2560 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 20s 92us/step - loss: 0.2500 - acc: 0.0000e+00 - val_loss: 0.2751 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 18s 85us/step - loss: 0.2482 - acc: 0.0000e+00 - val_loss: 0.2562 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 14s 65us/step - loss: 0.2480 - acc: 0.0000e+00 - val_loss: 0.2532 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 13s 61us/step - loss: 0.2477 - acc: 0.0000e+00 - val_loss: 0.2576 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 15s 68us/step - loss: 0.2420 - acc: 0.0000e+00 - val_loss: 0.2469 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 12s 56us/step - loss: 0.2416 - acc: 0.0000e+00 - val_loss: 0.2650 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 11s 51us/step - loss: 0.2417 - acc: 0.0000e+00 - val_loss: 0.2457 - val_acc: 0.0000e+00\n",
      "Train on 212897 samples, validate on 37571 samples\n",
      "Epoch 1/3\n",
      "212897/212897 [==============================] - 11s 52us/step - loss: 0.2399 - acc: 0.0000e+00 - val_loss: 0.2488 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "212897/212897 [==============================] - 11s 54us/step - loss: 0.2402 - acc: 0.0000e+00 - val_loss: 0.2427 - val_acc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "212897/212897 [==============================] - 11s 51us/step - loss: 0.2394 - acc: 0.0000e+00 - val_loss: 0.2480 - val_acc: 0.0000e+00\n",
      "Model4 trained\n",
      "RMSE NeuralNetwork:  0.48606080239953264\n",
      "Model predictions done.\n"
     ]
    }
   ],
   "source": [
    "nn_model = get_nn_complete_model(train, hidden2_neurons=12)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(log_dir='./simple_nn_logs', histogram_freq=0,\n",
    "                                 write_graph=True, write_images=True, )\n",
    "\n",
    "for i in range(5):\n",
    "    nn_model.fit(nn_train[0], nn_train[1], epochs=3, verbose=1,\n",
    "        batch_size=256, shuffle=True, validation_split=0.15, callbacks=[tb])\n",
    "    nn_model.fit(nn_train[0], nn_train[1], epochs=8, verbose=0,\n",
    "        batch_size=256, shuffle=True, callbacks=[tb])\n",
    "print(\"Model4 trained\")\n",
    "\n",
    "nn_preds = pd.Series(nn_model.predict(nn_train[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "print('RMSE NeuralNetwork: ', RMSLE(np.log1p(train['visitors'].values), nn_preds))\n",
    "\n",
    "nn_preds = pd.Series(nn_model.predict(nn_test[0]).reshape(-1)).clip(0, 6.8).values\n",
    "\n",
    "test['visitors'] = nn_preds\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "print(\"Model predictions done.\")\n",
    "# del train; del data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hklee\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../input/*.csv')}\n",
    "\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "date_info = data['hol'].copy()\n",
    "air_visit_data = data['tra'].copy()\n",
    "sample_submission = data['tes'].copy()\n",
    "    \n",
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "\n",
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='visit_date', how='left')\n",
    "visit_data.drop('visit_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['visit_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='visit_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values\n",
    "\n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub2 = sub2.fillna(-1) # for the unfound values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def final_visitors(x, alt=False):\n",
    "    visitors_x, visitors_y = x['visitors_x'], x['visitors_y']\n",
    "    if x['visitors_y'] == -1:\n",
    "        return visitors_x\n",
    "    else:\n",
    "        return 0.7*visitors_x + 0.3*visitors_y* 1.1\n",
    "\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n",
    "sub_merge['visitors'] = sub_merge.apply(lambda x: final_visitors(x), axis=1)\n",
    "print(\"Done\")\n",
    "sub_merge[['id', 'visitors']].to_csv('submissions/submission.csv.gz', compression='gzip', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
